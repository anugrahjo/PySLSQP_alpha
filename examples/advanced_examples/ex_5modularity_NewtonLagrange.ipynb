{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5\n",
    "An example written as a Python notebook (.ipynb) with minimal explanation in Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example 5'''\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from modopt.api import Optimizer\n",
    "from modopt.line_search_algorithms import ScipyLS, BacktrackingArmijo, Minpack2LS\n",
    "from modopt.merit_functions import AugmentedLagrangianEq, LagrangianEq\n",
    "# from modopt.approximate_hessians import BFGS, BFGSScipy, BFGSM1, Broyden, DFP, PSB, SR1\n",
    "from modopt.approximate_hessians import BFGSM1 as BFGS\n",
    "\n",
    "\n",
    "class NewtonLagrange(Optimizer):\n",
    "    def initialize(self):\n",
    "        self.solver_name = 'newton_lagrange'\n",
    "\n",
    "        self.obj = self.problem._compute_objective\n",
    "        self.grad = self.problem._compute_objective_gradient\n",
    "        self.con = self.problem._compute_constraints\n",
    "        self.jac = self.problem._compute_constraint_jacobian\n",
    "\n",
    "        self.options.declare('max_itr', default=1000, types=int)\n",
    "        self.options.declare('opt_tol', default=1e-8, types=float)\n",
    "        self.options.declare('feas_tol', default=1e-8, types=float)\n",
    "\n",
    "        self.default_outputs_format = {\n",
    "            'itr': int,\n",
    "            'obj': float,\n",
    "            # for arrays from each iteration, sizes need to be declared\n",
    "            'x': (float, (self.problem.nx, )),\n",
    "            'mult': (float, (self.problem.nc, )),\n",
    "            'con': (float, (self.problem.nc, )),\n",
    "            'opt': float,\n",
    "            'feas': float,\n",
    "            'time': float,\n",
    "            'num_f_evals': int,\n",
    "            'num_g_evals': int,\n",
    "            'step': float,\n",
    "            'merit': float,\n",
    "        }\n",
    "        self.options.declare('outputs',\n",
    "                             types=list,\n",
    "                             default=[\n",
    "                                 'itr', 'obj', 'x', 'mult', 'opt',\n",
    "                                 'feas', 'con', 'time', 'num_f_evals',\n",
    "                                 'num_g_evals', 'step', 'merit'\n",
    "                             ])\n",
    "\n",
    "    def setup(self):\n",
    "        nx = self.problem.nx\n",
    "        nc = self.problem.nc\n",
    "        self.QN = BFGS(nx=self.problem.nx)\n",
    "        self.MF = AugmentedLagrangianEq(nx=nx,\n",
    "                                        nc=nc,\n",
    "                                        f=self.obj,\n",
    "                                        c=self.con,\n",
    "                                        g=self.grad,\n",
    "                                        j=self.jac)\n",
    "\n",
    "        self.LS = BacktrackingArmijo(f=self.MF.compute_function,\n",
    "                                     g=self.MF.compute_gradient)\n",
    "        # self.LS = ScipyLS(f=self.MF.compute_function,\n",
    "        #                   g=self.MF.compute_gradient)\n",
    "\n",
    "        self.OF = LagrangianEq(nx=nx,\n",
    "                               nc=nc,\n",
    "                               f=self.obj,\n",
    "                               c=self.con,\n",
    "                               g=self.grad,\n",
    "                               j=self.jac)\n",
    "\n",
    "    def solve(self):\n",
    "        # Assign shorter names to variables and methods\n",
    "        nx = self.problem.nx\n",
    "        nc = self.problem.nc\n",
    "        x0 = self.problem.x.get_data()\n",
    "        opt_tol = self.options['opt_tol']\n",
    "        feas_tol = self.options['feas_tol']\n",
    "        max_itr = self.options['max_itr']\n",
    "        rho = 1.\n",
    "\n",
    "        obj = self.obj\n",
    "        grad = self.grad\n",
    "        con = self.con\n",
    "        jac = self.jac\n",
    "\n",
    "        LS = self.LS\n",
    "        QN = self.QN\n",
    "        MF = self.MF\n",
    "        OF = self.OF\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Set intial values for current iterates\n",
    "        x_k = x0 * 1.\n",
    "        f_k = obj(x_k)\n",
    "        g_k = grad(x_k)\n",
    "        c_k = con(x_k)\n",
    "        J_k = jac(x_k)\n",
    "        pi_k = np.full((nc, ), 0.)\n",
    "        v_k = np.concatenate((x_k, pi_k))\n",
    "        x_k = v_k[:nx]\n",
    "        pi_k = v_k[nx:]\n",
    "\n",
    "        # L_k = OF.evaluate_function(x_k, pi_k, f_k, c_k)\n",
    "        gL_k = OF.evaluate_gradient(x_k, pi_k, f_k, c_k, g_k, J_k)\n",
    "\n",
    "        MF.set_rho(rho)\n",
    "        mf_k = MF.evaluate_function(x_k, pi_k, f_k, c_k)\n",
    "        mfg_k = MF.evaluate_gradient(x_k, pi_k, f_k, c_k, g_k, J_k)\n",
    "\n",
    "        # Iteration counter\n",
    "        itr = 0\n",
    "\n",
    "        opt = np.linalg.norm(gL_k[:nx])\n",
    "        feas = np.linalg.norm(c_k)\n",
    "        num_f_evals = 1\n",
    "        num_g_evals = 1\n",
    "\n",
    "        # Initializing declared outputs\n",
    "        self.update_outputs(itr=0,\n",
    "                            x=x_k,\n",
    "                            mult=pi_k,\n",
    "                            obj=f_k,\n",
    "                            con=c_k,\n",
    "                            opt=opt,\n",
    "                            feas=feas,\n",
    "                            time=time.time() - start_time,\n",
    "                            num_f_evals=num_f_evals,\n",
    "                            num_g_evals=num_g_evals,\n",
    "                            step=0.,\n",
    "                            merit=mf_k)\n",
    "\n",
    "        while ((opt > opt_tol or feas > feas_tol) and itr < max_itr):\n",
    "            itr_start = time.time()\n",
    "            itr += 1\n",
    "\n",
    "            # Hessian approximation\n",
    "            B_k = QN.B_k\n",
    "\n",
    "            # ALGORITHM STARTS HERE\n",
    "            # >>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "            # Assemble KKT system\n",
    "            A = np.block([[B_k, -J_k.T], [-J_k, np.zeros((nc, nc))]])\n",
    "            b = -gL_k\n",
    "\n",
    "            # Compute the SEARCH DIRECTION toward the next iterate\n",
    "            p_k = np.linalg.solve(A, b)\n",
    "\n",
    "            # Compute the STEP LENGTH along the search direction via a LINE SEARCH\n",
    "            alpha, mf_k, new_f_evals, new_g_evals, converged = LS.search(\n",
    "                x=v_k, p=p_k, f0=mf_k, g0=mfg_k)\n",
    "\n",
    "            num_f_evals += new_f_evals\n",
    "            num_g_evals += new_g_evals\n",
    "\n",
    "            # A step of length 1e-4 is taken along p_k if line search does not converge\n",
    "            if not converged:\n",
    "                alpha = None\n",
    "                d_k = p_k * 1.\n",
    "\n",
    "            else:\n",
    "                d_k = alpha * p_k\n",
    "\n",
    "            v_k += d_k\n",
    "\n",
    "            # Update previous Lag. gradient with new lag. mult.s\n",
    "            # before updating the previous values\n",
    "            gL_k[:nx] += np.dot(J_k.T, (v_k[nx:] - pi_k))\n",
    "\n",
    "            f_k = obj(x_k)\n",
    "            g_k = grad(x_k)\n",
    "            c_k = con(x_k)\n",
    "            J_k = jac(x_k)\n",
    "\n",
    "            gL_k_new = OF.evaluate_gradient(x_k, pi_k, f_k, c_k, g_k, J_k)\n",
    "            w_k = (gL_k_new - gL_k)\n",
    "            gL_k = gL_k_new\n",
    "            mf_k = MF.evaluate_function(x_k, pi_k, f_k, c_k)\n",
    "            mfg_k = MF.evaluate_gradient(x_k, pi_k, f_k, c_k, g_k, J_k)\n",
    "\n",
    "            opt = np.linalg.norm(gL_k[:nx])\n",
    "            feas = np.linalg.norm(c_k)\n",
    "\n",
    "            # Update the Hessian approximation\n",
    "            QN.update(d_k[:nx], w_k[:nx])\n",
    "\n",
    "            # <<<<<<<<<<<<<<<<<<<\n",
    "            # ALGORITHM ENDS HERE\n",
    "\n",
    "            # Update arrays inside outputs dict with new values from the current iteration\n",
    "            self.update_outputs(itr=itr,\n",
    "                                x=x_k,\n",
    "                                mult=pi_k,\n",
    "                                obj=f_k,\n",
    "                                con=c_k,\n",
    "                                opt=opt,\n",
    "                                feas=feas,\n",
    "                                time=time.time() - start_time,\n",
    "                                num_f_evals=num_f_evals,\n",
    "                                num_g_evals=num_g_evals,\n",
    "                                step=alpha,\n",
    "                                merit=mf_k)\n",
    "\n",
    "        # Run post-processing for the Optimizer() base class\n",
    "        self.run_post_processing()\n",
    "\n",
    "        end_time = time.time()\n",
    "        self.total_time = end_time - start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
