{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4\n",
    "\n",
    "An example written as a Python notebook (.ipynb) with minimal explanation in Markdown\n",
    "\n",
    "We start this script by defining the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example 4'''\n",
    "import numpy as np\n",
    "from modopt.api import Problem\n",
    "\n",
    "\n",
    "class X4(Problem):\n",
    "    def initialize(self, ):\n",
    "        # Name your problem\n",
    "        self.problem_name = 'x^4'\n",
    "\n",
    "    def setup(self):\n",
    "        # Add design variables of your problem\n",
    "        self.add_design_variables('x',\n",
    "                                  shape=(2, ),\n",
    "                                  vals=np.array([.3, .3]))\n",
    "        self.add_objective('f',)\n",
    "\n",
    "    def setup_derivatives(self):\n",
    "        # Declare objective gradient and its shape\n",
    "        self.declare_objective_gradient(wrt='x', )\n",
    "        self.declare_objective_hessian(of='x', wrt='x')\n",
    "\n",
    "    # Compute the value of the objective, gradient and Hessian \n",
    "    # with the given design variable values\n",
    "    def compute_objective(self, dvs, obj):\n",
    "        obj['f'] = np.sum(dvs['x']**4)\n",
    "\n",
    "    def compute_objective_gradient(self, dvs, grad):\n",
    "        grad['x'] = 4 * dvs['x']**3\n",
    "\n",
    "    def compute_objective_hessian(self, dvs, hess):\n",
    "        hess['x', 'x'] = 12 * np.diag(dvs['x']**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build the steepest descent optimization algorithm using the optimizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from modopt.api import Optimizer\n",
    "\n",
    "class SteepestDescent(Optimizer):\n",
    "    def initialize(self):\n",
    "\n",
    "        # Name your algorithm\n",
    "        self.solver_name = 'steepest_descent'\n",
    "\n",
    "        self.obj = self.problem._compute_objective\n",
    "        self.grad = self.problem._compute_objective_gradient\n",
    "\n",
    "        self.options.declare('max_itr', default=1000, types=int)\n",
    "        self.options.declare('opt_tol', types=float)\n",
    "\n",
    "        # Specify format of outputs available from your optimizer after each iteration\n",
    "        self.default_outputs_format = {\n",
    "            'itr': int,\n",
    "            'obj': float,\n",
    "            # for arrays from each iteration, shapes need to be declared\n",
    "            'x': (float, (self.problem.nx, )),\n",
    "            'opt': float,\n",
    "            'time': float,\n",
    "        }\n",
    "\n",
    "        # Enable user to specify, as a list, which among the available outputs\n",
    "        # need to be stored in memory and written to output files\n",
    "        self.options.declare('outputs',\n",
    "                             types=list,\n",
    "                             default=['itr', 'obj', 'x', 'opt', 'time'])\n",
    "\n",
    "    def solve(self):\n",
    "        nx = self.problem.nx\n",
    "        x = self.problem.x.get_data()\n",
    "        opt_tol = self.options['opt_tol']\n",
    "        max_itr = self.options['max_itr']\n",
    "\n",
    "        obj = self.obj\n",
    "        grad = self.grad\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Setting intial values for initial iterates\n",
    "        x_k = x * 1.\n",
    "        f_k = obj(x_k)\n",
    "        g_k = grad(x_k)\n",
    "\n",
    "        # Iteration counter\n",
    "        itr = 0\n",
    "\n",
    "        # Optimality\n",
    "        opt = np.linalg.norm(g_k)\n",
    "\n",
    "        # Initializing outputs\n",
    "        self.update_outputs(itr=0,\n",
    "                            x=x_k,\n",
    "                            obj=f_k,\n",
    "                            opt=opt,\n",
    "                            time=time.time() - start_time)\n",
    "\n",
    "        while (opt > opt_tol and itr < max_itr):\n",
    "            itr_start = time.time()\n",
    "            itr += 1\n",
    "\n",
    "            # ALGORITHM STARTS HERE\n",
    "            # >>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "            p_k = -g_k\n",
    "\n",
    "            x_k += p_k\n",
    "            f_k = obj(x_k)\n",
    "            g_k = grad(x_k)\n",
    "\n",
    "            opt = np.linalg.norm(g_k)\n",
    "\n",
    "            # <<<<<<<<<<<<<<<<<<<\n",
    "            # ALGORITHM ENDS HERE\n",
    "\n",
    "            # Append arrays inside outputs dict with new values from the current iteration\n",
    "            self.update_outputs(itr=itr,\n",
    "                                x=x_k,\n",
    "                                obj=f_k,\n",
    "                                opt=opt,\n",
    "                                time=time.time() - start_time)\n",
    "\n",
    "        # Run post-processing for the Optimizer() base class\n",
    "        self.run_post_processing()\n",
    "\n",
    "        end_time = time.time()\n",
    "        self.total_time = end_time - start_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up your optimizer with the problem defined above to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting objective name as \"f\".\n",
      "Directory  x^4_outputs  already exists\n",
      "Directory  x^4_outputs  already exists\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Derivative type | Calc norm  | FD norm    | Abs error norm | Rel error norm \n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "Gradient        | 1.5274e-01 | 1.5274e-01 | 7.6367e-07     | 7.0710e-06    \n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " \t ===============================\n",
      "\t ModOpt final iteration summary:\n",
      "\t ===============================\n",
      "\t Problem           : x^4\n",
      "\t Solver            : bfgs\n",
      "\t itr               : 20\n",
      "\t obj               : 1.6785177544170953e-12\n",
      "\t opt               : 4.9601744115117986e-09\n",
      "\t time              : 0.403872013092041\n",
      "\t num_f_evals       : 21\n",
      "\t num_g_evals       : 21\n",
      "\t step              : 1.0\n",
      "\t =========================================\n",
      "\n",
      "\n",
      "==================================================================\n",
      "                      modOpt summary table:                       \n",
      "==================================================================\n",
      " itr      obj      opt     time  num_f_evals  num_g_evals     step\n",
      "   0 1.62E-02 1.53E-01 5.67E-05            1            1 0.00E+00\n",
      "   2 1.11E-03 2.05E-02 3.28E-02            3            3 1.00E+00\n",
      "   4 1.11E-04 3.64E-03 6.53E-02            5            5 1.00E+00\n",
      "   6 1.16E-05 6.69E-04 8.97E-02            7            7 1.00E+00\n",
      "   8 1.22E-06 1.24E-04 1.18E-01            9            9 1.00E+00\n",
      "  11 4.18E-08 9.84E-06 1.90E-01           12           12 1.00E+00\n",
      "  13 4.41E-09 1.82E-06 2.28E-01           14           14 1.00E+00\n",
      "  15 4.65E-10 3.37E-07 2.96E-01           16           16 1.00E+00\n",
      "  17 4.90E-11 6.23E-08 3.43E-01           18           18 1.00E+00\n",
      "  20 1.68E-12 4.96E-09 4.04E-01           21           21 1.00E+00\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set your optimality tolerance\n",
    "opt_tol = 1E-8\n",
    "# Set maximum optimizer iteration limit\n",
    "max_itr = 100\n",
    "\n",
    "prob = X4()\n",
    "\n",
    "from modopt.optimization_algorithms import Newton, QuasiNewton, SQP\n",
    "\n",
    "# Set up your optimizer with your problem and pass in optimizer parameters\n",
    "optimizer = SteepestDescent(prob,\n",
    "                            opt_tol=opt_tol,\n",
    "                            max_itr=max_itr,\n",
    "                            outputs=['itr', 'obj', 'x', 'opt', 'time'])\n",
    "optimizer = Newton(prob, opt_tol=opt_tol)\n",
    "optimizer = QuasiNewton(prob, opt_tol=opt_tol)\n",
    "\n",
    "# Check first derivatives at the initial guess, if needed\n",
    "optimizer.check_first_derivatives(prob.x.get_data())\n",
    "\n",
    "# Solve your optimization problem\n",
    "optimizer.solve()\n",
    "\n",
    "# Print results of optimization (summary_table contains information from each iteration)\n",
    "optimizer.print_results(summary_table=True, compact_print=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print to console to see any of the outputs that were declared.\n",
    "Since the arrays are long, here we only print the last entry and verify it with the print_results() above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Optimizer data\n",
      "num_iterations: 20\n",
      "optimized_dvs: [0.00095711 0.00095716]\n",
      "optimization_time: 0.403872013092041\n",
      "optimized_obj: 1.6785177544170953e-12\n",
      "final_optimality: 4.9601744115117986e-09\n",
      "\n",
      "\n",
      "Final problem data\n",
      "optimized_dvs: [0.00095711 0.00095716]\n",
      "optimized_obj: 1.6785177544170953e-12\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('Optimizer data')\n",
    "print('num_iterations:', optimizer.outputs['itr'][-1])\n",
    "print('optimized_dvs:', optimizer.outputs['x'][-1])\n",
    "print('optimization_time:', optimizer.outputs['time'][-1])\n",
    "print('optimized_obj:', optimizer.outputs['obj'][-1])\n",
    "print('final_optimality:', optimizer.outputs['opt'][-1])\n",
    "\n",
    "print('\\n')\n",
    "print('Final problem data')\n",
    "print('optimized_dvs:', prob.x.get_data())\n",
    "print('optimized_obj:', prob.obj['f'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
